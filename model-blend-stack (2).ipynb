{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9261388,"sourceType":"datasetVersion","datasetId":5603931},{"sourceId":9267468,"sourceType":"datasetVersion","datasetId":5608256}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xgboost\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import matthews_corrcoef,accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score, f1_score, precision_score\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\nwarnings.simplefilter(action='ignore', category=Warning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\nimport gdown\n# File ID from the Google Drive link\nfile_id = '1LgDTq4lF5FskiWkyAu7a4hMYSuhFCV-l'\ndestination = 'mushroom_train.csv'  # Replace with your desired file name and extension\ngdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n\n# File ID from the Google Drive link\nfile_id = '1dlWqwY_0Pg3h8eZyQ80kkH6htpQn5W6o'\ndestination = 'mushroom_test.csv'  # Replace with your desired file name and extension\ngdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n\n# File ID from the Google Drive link\nfile_id = '1-E9vUgsplqT3rc2_yFIQZE9O4cMi-Fo0'\ndestination = 'mushroom_sample_submission.csv'  # Replace with your desired file name and extension\ngdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n\n# File ID from the Google Drive link\nfile_id = '18oSB_b0ub_htk9XWHbSaWmYVrZLU6NQJ'\ndestination = 'new_data.csv'  # this is secondary mushroom dataset combined rows with compitition dataset\ngdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv('mushroom_train.csv')\ndf_test = pd.read_csv('mushroom_test.csv')\ndf_sample_submission = pd.read_csv('mushroom_sample_submission.csv')\ndf= pd.read_csv(\"new_data.csv\")\nids = df_test['id']\ntrain_df = train_df.drop(columns=['id'], axis=1)\ndf_test = df_test.drop(columns=['id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df_train is your training DataFrame and `class` is the target feature\n# Assuming that each categorical feature is already converted to 'category' dtype\n\n# List of categorical columns (excluding the target column 'class')\ncategorical_columns = df.select_dtypes(include=['object']).columns.difference(['class'])\n\n# Function to calculate the percentage of each class label ('e', 'p') for each label in the feature\ndef calculate_class_percentage(df, feature, target='class'):\n    percentages = df.groupby([feature, target]).size().unstack().fillna(0)\n    percentages = percentages.div(percentages.sum(axis=1), axis=0) * 100\n    return percentages\n\n# Iterate through each categorical column and calculate the percentages\nfor feature in categorical_columns:\n    print(f\"Class percentages for feature '{feature}':\")\n    percentages = calculate_class_percentage(df, feature)\n    print(percentages)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize dictionaries to store the results for each feature\nunknown_e = {}\nunknown_p = {}\n\n# Features to print full rows for\nfeatures_to_print = ['cap-shape', 'cap-surface', 'cap-color',\n    'does-bruise-or-bleed', 'gill-attachment',\n    'gill-spacing', 'gill-color', 'stem-root',\n    'stem-surface', 'stem-color', 'veil-type',\n    'veil-color', 'has-ring', 'ring-type',\n    'spore-print-color', 'habitat', 'season']\n\n# Function to calculate the percentage of each class label ('e', 'p') for each label in the feature\ndef calculate_class_percentage(df, feature, target='class'):\n    percentages = df.groupby([feature, target]).size().unstack().fillna(0)\n    percentages = percentages.div(percentages.sum(axis=1), axis=0) * 100\n    return percentages\n\n# Iterate through each categorical column and calculate the percentages\nfor feature in features_to_print:\n    if feature in features_to_print:\n        percentages = calculate_class_percentage(df, feature)\n\n        # Extract labels where the percentage is 100 for class 'e'\n        unknown_e[feature] = percentages[percentages['e'] == 100].index.tolist()\n\n        # Extract labels where the percentage is 100 for class 'p'\n        unknown_p[feature] = percentages[percentages['p'] == 100].index.tolist()\n\n# Create and assign dynamic variables for each feature\nfor feature in features_to_print:\n    # Define variable names dynamically\n    var_name_e = f\"{feature}_unknown_e\"\n    var_name_p = f\"{feature}_unknown_p\"\n\n    # Assign the lists to dynamically created variable names\n    globals()[var_name_e] = unknown_e.get(feature, [])\n    globals()[var_name_p] = unknown_p.get(feature, [])\n\n# Print the dynamically created variables to verify\nfor feature in features_to_print:\n    var_name_e = f\"{feature}_unknown_e\"\n    var_name_p = f\"{feature}_unknown_p\"\n    print(f\"{var_name_e}: {globals().get(var_name_e)}\")\n    print(f\"{var_name_p}: {globals().get(var_name_p)}\")\n    print()\n\n# Create replacement mappings for each feature\nreplacement_mappings = {}\nfor feature in features_to_print:\n    var_name_e = f\"{feature}_unknown_e\"\n    var_name_p = f\"{feature}_unknown_p\"\n\n    # Create replacement mappings only if there are unknown labels\n    if globals().get(var_name_e):\n        replacement_mapping_e = {label: 'unknown-e' for label in globals().get(var_name_e)}\n        replacement_mappings[feature] = replacement_mapping_e\n        df[feature] = df[feature].replace(replacement_mapping_e)\n        df_test[feature] = df_test[feature].replace(replacement_mapping_e)\n\n    if globals().get(var_name_p):\n        replacement_mapping_p = {label: 'unknown-p' for label in globals().get(var_name_p)}\n        replacement_mappings.setdefault(feature, {}).update(replacement_mapping_p)\n        df[feature] = df[feature].replace(replacement_mapping_p)\n        df_test[feature] = df_test[feature].replace(replacement_mapping_p)\n\n# If you want to verify the final replacement mappings\nprint(replacement_mappings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns to encode\ncolumns_to_encode = [\n    'cap-shape', 'cap-surface', 'cap-color', 'gill-spacing', 'gill-attachment',\n    'does-bruise-or-bleed', 'gill-color', 'season', 'habitat', 'spore-print-color',\n    'ring-type', 'has-ring', 'veil-color', 'veil-type', 'stem-surface', 'stem-root','stem-color'\n]\n\n# Convert all values to strings and fill NaN with '-1'\ndf[columns_to_encode] = df[columns_to_encode].astype(str).fillna('-1')\ndf_test[columns_to_encode] = df_test[columns_to_encode].astype(str).fillna('-1')\n\n# Initialize LabelEncoder\nle = LabelEncoder()\n\n# Encode the features for both df and test_data\nfor col in columns_to_encode:\n    combined_data = pd.concat([df[col], df_test[col]], axis=0)  # Combine df and test_data\n    le.fit(combined_data)  # Fit on the combined data\n    df[col] = le.transform(df[col])\n    df_test[col] = le.transform(df_test[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.fillna(-1, inplace=True)\ndf_test.fillna(-1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class'] = df['class'].replace('e',0)\ndf['class'] = df['class'].replace('p',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = 'class'\nX = df.drop(columns=[label], axis=1)\ny = df[label]\n\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### LGBM\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import matthews_corrcoef\nimport pandas as pd\nimport numpy as np\n\n# Best parameters for LightGBM\nbest_params = {\n    'num_leaves': 227, 'learning_rate': 0.06737922153052582, 'n_estimators': 357, 'subsample_for_bin': 213892, 'min_child_samples': 292, 'reg_alpha': 4.5070762908503614e-07, 'reg_lambda': 0.00017079711660762664, 'colsample_bytree': 0.45636430645506504, 'subsample': 0.6819598716671345, 'max_depth': 14,'device':'GPU', 'verbosity':-1,\n    'n_jobs': -1,\n    'gpu_platform_id': 0,  # Adjust if necessary\n    'gpu_device_id': 0  # Adjust if necessary\n}\n\n# Initialize LightGBM with the best parameters\nbest_model2 = lgb.LGBMClassifier(**best_params)\n\n# Initialize arrays to hold OOF predictions\noof_preds_lgbm = np.zeros(len(X))\n\n# Perform 4-fold stratified cross-validation for OOF predictions\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\nfor train_ind, valid_ind in skf.split(X, y):\n    X_train, X_val = X.iloc[train_ind], X.iloc[valid_ind]\n    y_train, y_val = y.iloc[train_ind], y.iloc[valid_ind]\n    \n    # Train the model\n    best_model2.fit(X_train, y_train)\n    \n    # Predict probabilities and generate OOF predictions\n    oof_preds_lgbm[valid_ind] = best_model2.predict_proba(X_val)[:, 1]\n\n# Convert OOF probabilities to binary class labels\noof_preds_binary = (oof_preds_lgbm > 0.5).astype(int)\n\n# Calculate the MCC score\nmcc = matthews_corrcoef(y, oof_preds_binary)\nprint(f\"Matthews Correlation Coefficient (OOF): {mcc}\")\n\n# Re-train the model with best parameters on the entire training data\nbest_model2.fit(X, y)\n\n# Print feature importances\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': best_model2.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nprint(\"\\nFeature importances for LightGBM:\")\nprint(importance_df.head(20))\n\n# Predict on test data\ntest_pred_prob = best_model2.predict_proba(df_test)[:, 1]\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_pred_prob > 0.5).astype(int)\n\n# Prepare submission\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_lgbm.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file to modify\ndf3 = pd.read_csv(output_file)\n\n# Drop the 'class' column if it exists\nif 'class' in df3.columns:\n    df3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new12_lgbm.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n#Matthews Correlation Coefficient (OOF): 0.9848833003490562","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Matthews Correlation Coefficient (OOF): 0.9848833003490562  , cv=10\n\nSubmission and Description\n\nPublic Score\n\nSelect\n\nfinal_predictions_no_class_new12_lgbm (1).csv\nComplete · 30s ago\n0.98487\n\n","metadata":{}},{"cell_type":"code","source":"### LGBM\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import matthews_corrcoef\nimport pandas as pd\nimport numpy as np\n\n# Best parameters for LightGBM\nbest_params = {\n  'num_leaves': 929, 'learning_rate': 0.010704218899215131, 'n_estimators': 2492, 'subsample_for_bin': 263922, 'min_child_samples': 58, 'reg_alpha': 0.11039306751778567, 'reg_lambda': 4.8106602020701585e-09, 'colsample_bytree': 0.34381132307476014, 'subsample': 0.8996454730099417, 'max_depth': 14,'device':'GPU',\n    'n_jobs': -1,\n    'gpu_platform_id': 0,  # Adjust if necessary\n    'gpu_device_id': 0  # Adjust if necessary \n}\n    \n    \n\n# Initialize LightGBM with the best parameters\nbest_model2 = lgb.LGBMClassifier(**best_params)\n\n# Initialize arrays to hold OOF predictions\noof_preds_lgbm = np.zeros(len(X))\n\n# Perform 4-fold stratified cross-validation for OOF predictions\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\nfor train_ind, valid_ind in skf.split(X, y):\n    X_train, X_val = X.iloc[train_ind], X.iloc[valid_ind]\n    y_train, y_val = y.iloc[train_ind], y.iloc[valid_ind]\n    \n    # Train the model\n    best_model2.fit(X_train, y_train)\n    \n    # Predict probabilities and generate OOF predictions\n    oof_preds_lgbm[valid_ind] = best_model2.predict_proba(X_val)[:, 1]\n\n# Convert OOF probabilities to binary class labels\noof_preds_binary = (oof_preds_lgbm > 0.5).astype(int)\n\n# Calculate the MCC score\nmcc = matthews_corrcoef(y, oof_preds_binary)\nprint(f\"Matthews Correlation Coefficient (OOF): {mcc}\")\n\n# Re-train the model with best parameters on the entire training data\nbest_model2.fit(X, y)\n\n# Print feature importances\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': best_model2.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nprint(\"\\nFeature importances for LightGBM:\")\nprint(importance_df.head(20))\n\n# Predict on test data\ntest_pred_prob = best_model2.predict_proba(df_test)[:, 1]\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_pred_prob > 0.5).astype(int)\n\n# Prepare submission\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_lgbm.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file to modify\ndf3 = pd.read_csv(output_file)\n\n# Drop the 'class' column if it exists\nif 'class' in df3.columns:\n    df3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new12_lgbm.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n#Matthews Correlation Coefficient (OOF): 0.9848833003490562","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"above lgbm with 10 cv and with best params i got lb score as 98503\n\nbest_params = {\n  'num_leaves': 929, 'learning_rate': 0.010704218899215131, 'n_estimators': 2492, 'subsample_for_bin': 263922, 'min_child_samples': 58, 'reg_alpha': 0.11039306751778567, 'reg_lambda': 4.8106602020701585e-09, 'colsample_bytree': 0.34381132307476014, 'subsample': 0.8996454730099417, 'max_depth': 14,'device':'GPU',\n    'n_jobs': -1,\n    'gpu_platform_id': 0,  # Adjust if necessary\n    'gpu_device_id': 0  # Adjust if necessary \n}","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import matthews_corrcoef\nfrom xgboost import XGBClassifier\nimport pandas as pd\n\n# Best parameters found from Optuna (from previous runs)\nbest_params = {\n   \n    'enable_categorical': True,\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'n_estimators': 360,         \n    'learning_rate': 0.1,           \n    'max_depth': 17,                \n    'colsample_bytree': 0.4,         \n    'min_child_weight': 2,           \n    'reg_lambda': 67,                \n    'subsample': 0.98,              \n    'num_parallel_tree': 4,\n}\n\n# Initialize XGBoost with the best parameters\nbest_model1 = XGBClassifier(**best_params)\n\n# Initialize arrays to hold OOF predictions\noof_preds_xgboost = np.zeros(len(X))\n\n# Perform 4-fold stratified cross-validation for OOF predictions\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=9)\n\nfor train_ind, valid_ind in skf.split(X, y):\n    X_train, X_val = X.iloc[train_ind], X.iloc[valid_ind]\n    y_train, y_val = y.iloc[train_ind], y.iloc[valid_ind]\n    \n    # Train the model\n    best_model1.fit(X_train, y_train)\n    \n    # Predict probabilities and generate OOF predictions\n    oof_preds_xgboost[valid_ind] = best_model1.predict_proba(X_val)[:, 1]\n\n# Convert OOF probabilities to binary class labels\noof_preds_binary = (oof_preds_xgboost > 0.5).astype(int)\n\n# Calculate the MCC score\nmcc = matthews_corrcoef(y, oof_preds_binary)\nprint(f\"Matthews Correlation Coefficient (OOF): {mcc}\")\n\n# Re-train the model with best parameters on the entire training data\nbest_model1.fit(X, y)\n\n# Print feature importances\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': best_model1.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nprint(\"\\nFeature importances for XGBoost:\")\nprint(importance_df.head(20))\n\n# Predict on test data\ntest_pred_prob = best_model1.predict_proba(df_test)[:, 1]\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_pred_prob > 0.5).astype(int)\n\n# Prepare submission\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_xgboost.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file to modify\ndf3 = pd.read_csv(output_file)\n\n# Drop the 'class' column if it exists\nif 'class' in df3.columns:\n    df3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new12.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n# Matthews Correlation Coefficient (OOF): 0.9851386851107209\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgboost  cv=10\nbest_params = {\n   \n    'enable_categorical': True,\n    'tree_method': 'hist',\n    'device': 'cuda',\n    'n_estimators': 360,         \n    'learning_rate': 0.1,           \n    'max_depth': 17,                \n    'colsample_bytree': 0.4,         \n    'min_child_weight': 2,           \n    'reg_lambda': 67,                \n    'subsample': 0.98,              \n    'num_parallel_tree': 4,\n}\n\nMatthews Correlation Coefficient (OOF): 0.9851386851107209  , cv=10\n\nSubmission and Description\n\nPublic Score\n\nSelect\n\nfinal_predictions_no_class_new12 (14).csv\nComplete · 28s ago\n0.98508","metadata":{}},{"cell_type":"code","source":"# Step 4: Create Meta-Model Input Data\n\n\noof_df = pd.DataFrame({\n     'lgbm_oof': trainset_oof['lgbm_oof'],\n    'xgb_oof': trainset_oof['xgb_oof'],\n    'catboost_oof': oof_preds_catboost,\n    'target': y\n})\n\n\n# Optionally, inspect the created DataFrame\nprint(oof_df.head())\n\n# Create input features (X_meta) and target (y_meta) for the meta-model\nX_meta = oof_df[['lgbm_oof', 'xgb_oof','catboost_oof']]\ny_meta = oof_df['target']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame as a CSV file\noof_df.to_csv('10_cv_lgbm_xgboost_catboost_oof_train_set_csv.csv', index=False)\nx=pd.read_csv('10_cv_lgbm_xgboost_catboost_oof_train_set_csv.csv')\nx.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.linear_model import ElasticNet\nimport optuna\n\n# Step 4: Hyperparameter Tuning for Meta-Model (ElasticNet) with Optuna\ndef objective(trial):\n    alpha = trial.suggest_float('alpha', 1e-6, 1.0, log=True)\n    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n    max_iter = trial.suggest_int('max_iter', 100, 2000)\n\n    meta_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, random_state=9)\n\n    skf_meta = StratifiedKFold(n_splits=4, shuffle=True, random_state=9)\n    meta_pred_oof = cross_val_predict(meta_model, X_meta, y_meta, cv=skf_meta, method='predict')\n    meta_pred_binary = (meta_pred_oof > 0.5).astype(int)\n\n    mcc = matthews_corrcoef(y_meta, meta_pred_binary)\n\n    return mcc\n\n# Run Optuna optimization\n# Create a TPE sampler with a fixed seed\n#tpe_sampler = optuna.samplers.TPESampler(seed=42)\n\n# Create the study with the specified sampler\n#study = optuna.create_study(direction='maximize', sampler=tpe_sampler)\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Retrieve the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters for the meta model: {best_params}\")\n\n# Step 5: Train Meta-Model with the Best Hyperparameters\nbest_meta_model1 = ElasticNet(**best_params, random_state=9)\nbest_meta_model1.fit(X_meta, y_meta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## working on\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nimport optuna\nfrom sklearn.metrics import matthews_corrcoef\n\n# Step 4: Hyperparameter Tuning for Meta-Model (Logistic Regression) with Optuna\ndef objective(trial):\n    C = trial.suggest_float('C', 1e-6, 10.0, log=True)  # Inverse of regularization strength\n    max_iter = trial.suggest_int('max_iter', 100, 1000)  # Maximum iterations\n    solver = trial.suggest_categorical('solver', ['liblinear', 'saga', 'lbfgs'])  # Solvers\n\n    # Create the Logistic Regression model with sampled parameters\n    meta_model = LogisticRegression(C=C, max_iter=max_iter, solver=solver, random_state=9)\n\n    # Stratified K-Fold cross-validation\n    skf_meta = StratifiedKFold(n_splits=4, shuffle=True, random_state=9)\n    meta_pred_oof = cross_val_predict(meta_model, X_meta, y_meta, cv=skf_meta, method='predict')\n\n    # Calculate MCC\n    mcc = matthews_corrcoef(y_meta, meta_pred_oof)\n    return mcc\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Retrieve the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters for the meta model: {best_params}\")\n\n# Step 5: Train Meta-Model with the Best Hyperparameters\nbest_meta_model2 = LogisticRegression(**best_params, random_state=9)\nbest_meta_model2.fit(X_meta, y_meta)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 7: Generate Predictions on Test Set\n#xgb1_test_oof = best_model1.predict_proba(df_test)[:, 1]\n#lgbm_test_oof = best_model2.predict_proba(df_test)[:, 1]\noof_test_preds_catboost = model4.predict_proba(df_test)[:, 1]\n\ntest_meta_df = pd.DataFrame({\n     'lgbm_oof': testset_oof['lgbm_oof'],\n    'xgb_oof': testset_oof['xgb_oof'],\n    'catboost_oof': oof_test_preds_catboost\n})\n\n# Standardize the test meta data\n#test_meta_scaled = scaler.transform(test_meta_df)\n#test_predictions_prob = elastic_net_model.predict_proba(test_meta_df)[:, 1]\n\ntest_predictions_prob = best_meta_model2.predict(test_meta_df)\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_predictions_prob > 0.5).astype(int)\n\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_xgboost_stacked_model.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file\ndf3 = pd.read_csv('final_predictions_xgboost_stacked_model.csv')\n\n# Drop the 'class' column\ndf3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new_xg_boost_stacked1234.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame as a CSV file\ntest_meta_df.to_csv('10_cv_lgbm_xgboost_catboost_oof_test_set_csv.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=pd.read_csv('10_cv_lgbm_xgboost_catboost_oof_test_set_csv.csv')\nx.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_meta_df = pd.DataFrame({\n     'lgbm_oof': testset_oof['lgbm_oof'],\n    'xgb_oof': testset_oof['xgb_oof'],\n    'catboost_oof': oof_preds_catboost\n})\n\ntest_predictions_prob = best_meta_model2.predict(test_meta_df)\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_predictions_prob > 0.5).astype(int)\n\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_xgboost_stacked_model.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file\ndf3 = pd.read_csv('final_predictions_xgboost_stacked_model.csv')\n\n# Drop the 'class' column\ndf3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new_xg_boost_stacked1234.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Performing the hyperparameter tuning with on lgbm which score lb score as 98497\n\n#LGBM OPTUNA TUNING MUST TRY because it will give 98497 lb score \n\ndef objective(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 400, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 1.0, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 2500),\n        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 20000, 300000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 500),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 100.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 100.0, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 15)\n    }\n    \n    model = LGBMClassifier(**params, random_state=0, objective='binary', verbosity = -1)\n    X, X_val, y, y_val = train_test_split(X_train, y_train, random_state=0, stratify=y_train, test_size=0.2, shuffle=True)\n    model.fit(X, y)\n    y_pred = model.predict(X_val)\n    return matthews_corrcoef(y_val, y_pred)\n    \n\nsqlite_db = \"sqlite:///lgbm.db\"\nstudy_name = \"lgbm\"\noptimize = False\nif optimize:\n    study = optuna.create_study(storage=sqlite_db, study_name=study_name, \n                                sampler=TPESampler(n_startup_trials=50, multivariate=True, seed=0),\n                                direction=\"maximize\", load_if_exists=True)\n\n    study.optimize(objective, n_trials=150)\n    print(f\"best optimized MCC: {study.best_value:0.5f}\") # 0.98521\n    print(f\"best hyperparameters: {study.best_params}\") \n    lgbm_params = study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom lightgbm import LGBMClassifier\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import matthews_corrcoef\nimport pandas as pd\n\n# Define the objective function for Optuna\ndef objective(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 400, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 1.0, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 2500),\n        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 20000, 300000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 500),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 100.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 100.0, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'device': 'GPU'\n    }\n    \n    model = LGBMClassifier(**params, random_state=0, objective='binary', verbosity=-1)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, stratify=y, test_size=0.2, shuffle=True)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    return matthews_corrcoef(y_val, y_pred)\n\n# Initialize Optuna study and optimize\nstudy = optuna.create_study(sampler=TPESampler(), direction='maximize')\nstudy.optimize(objective, n_trials=100)  # Adjust n_trials as needed\n\n# Retrieve best hyperparameters\nbest_params = study.best_params\n\n# Train the model on the full training set with the best hyperparameters\nbest_model = LGBMClassifier(**best_params, random_state=0, objective='binary', verbosity=-1)\nbest_model.fit(X, y)\n\n# Predict on test data\ntest_pred_prob = best_model.predict_proba(df_test)[:, 1]\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_pred_prob > 0.5).astype(int)\n\n# Prepare submission\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_lgbm.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file to modify\ndf3 = pd.read_csv(output_file)\n\n# Drop the 'class' column if it exists\nif 'class' in df3.columns:\n    df3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new12.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[I 2024-08-26 14:19:49,014] Trial 0 finished with value: 0.9848939633638614 and parameters: {'num_leaves': 730, 'learning_rate': 0.034046059791858485, 'n_estimators': 1513, 'subsample_for_bin': 176028, 'min_child_samples': 494, 'reg_alpha': 2.0417397933460876e-05, 'reg_lambda': 4.454406348639976e-06, 'colsample_bytree': 0.48663514847243605, 'subsample': 0.6018624080090369, 'max_depth': 15}. Best is trial 0 with value: 0.9848939633638614.\n\n\n[I 2024-08-26 23:09:13,798] Trial 33 finished with value: 0.9851800323425373 and parameters: {'num_leaves': 705, 'learning_rate': 0.014413440850606295, 'n_estimators': 2496, 'subsample_for_bin': 203526, 'min_child_samples': 285, 'reg_alpha': 2.7046833821279917e-05, 'reg_lambda': 7.38502567343188e-07, 'colsample_bytree': 0.3675170902152378, 'subsample': 0.6621847806842337, 'max_depth': 13}. Best is trial 33 with value: 0.9851800323425373.","metadata":{}},{"cell_type":"markdown","source":"### Working on 2 models for stacking","metadata":{}},{"cell_type":"code","source":"# Load your train and test OOF predictions\ntestset_oof = pd.read_csv(\"/kaggle/working/10_cv_lgbm_xgboost_oof_test_set_csv.csv\")\ntrainset_oof = pd.read_csv(\"/kaggle/working/10_cv_lgbm_xgboost_oof_train_set_csv.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Create input features (X_meta) and target (y_meta) for the meta-model\nX_meta = trainset_oof[['lgbm_oof', 'xgb_oof']]\ny_meta = trainset_oof['target']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## working on\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nimport optuna\nfrom sklearn.metrics import matthews_corrcoef\n\n# Step 4: Hyperparameter Tuning for Meta-Model (Logistic Regression) with Optuna\ndef objective(trial):\n    C = trial.suggest_float('C', 1e-6, 10.0, log=True)  # Inverse of regularization strength\n    max_iter = trial.suggest_int('max_iter', 100, 1000)  # Maximum iterations\n    solver = trial.suggest_categorical('solver', ['liblinear', 'saga', 'lbfgs'])  # Solvers\n\n    # Create the Logistic Regression model with sampled parameters\n    meta_model = LogisticRegression(C=C, max_iter=max_iter, solver=solver, random_state=9)\n\n    # Stratified K-Fold cross-validation\n    skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)\n    meta_pred_oof = cross_val_predict(meta_model, X_meta, y_meta, cv=skf_meta, method='predict')\n\n    # Calculate MCC\n    mcc = matthews_corrcoef(y_meta, meta_pred_oof)\n    return mcc\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Retrieve the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters for the meta model: {best_params}\")\n\n# Step 5: Train Meta-Model with the Best Hyperparameters\nbest_meta_model2 = LogisticRegression(**best_params, random_state=9)\nbest_meta_model2.fit(X_meta, y_meta)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_meta1=testset_oof[['lgbm_oof', 'xgb_oof']]\ntest_predictions_prob = best_meta_model2.predict(test_meta1)\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_predictions_prob > 0.5).astype(int)\n\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_xgboost_stacked_model.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file\ndf3 = pd.read_csv('final_predictions_xgboost_stacked_model.csv')\n\n# Drop the 'class' column\ndf3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = '_optuna_final_predictions_no_class_new_xg_boost_stacked12.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### catboost with best params \nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\n# Best parameters for LightGBM\nbest_params = {\n    'task_type': 'GPU','n_estimators': 5939, 'learning_rate': 0.011746226160973147, 'random_strength': 0.6406712628822828, 'max_bin': 71, 'depth': 11, 'l2_leaf_reg': 6.236947205167646, 'grow_policy': 'SymmetricTree', 'bootstrap_type': 'Bernoulli', 'subsample': 0.4963564081845081\n}\n\n# Initialize LightGBM with the best parameters\nbest_model9 = CatBoostClassifier(**best_params)\n\n# Train the model on the entire training data\nbest_model9.fit(X, y)\n\n# Print feature importances\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': best_model9.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nprint(\"\\nFeature importances for LightGBM:\")\nprint(importance_df.head(20))\n\n# Predict on test data\ntest_pred_prob = best_model9.predict_proba(df_test)[:, 1]\n\n# Convert probabilities to binary class labels\ntest_pred_binary = (test_pred_prob > 0.5).astype(int)\n\n# Prepare submission\ndf_sample_submission['predictions'] = test_pred_binary\ndf_sample_submission['predictions'] = df_sample_submission['predictions'].replace({0: 'e', 1: 'p'})\ndf_sample_submission.head()\n\n# Save the final DataFrame to CSV\noutput_file = 'final_predictions_lgbm.csv'\ndf_sample_submission.to_csv(output_file, index=False)\n\n# Load the CSV file to modify\ndf3 = pd.read_csv(output_file)\n\n# Drop the 'class' column if it exists\nif 'class' in df3.columns:\n    df3.drop(columns=['class'], inplace=True)\n\n# Save the updated DataFrame to a new CSV file\noutput_file = 'final_predictions_no_class_new12_lgbm.csv'\ndf3.to_csv(output_file, index=False)\n\nprint(f\"'class' column dropped. Updated file saved as '{output_file}'\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return the trained model and test predictions\nfrom catboost import CatBoostClassifier\ncatboost_model = CatBoostClassifier()\nmodel4=catboost_model.load_model(\"/kaggle/input/catboost-oofs-and-best-model/catboost_final_model.cbm\")\nprint(model4)\n# Print the model parameters\nprint(\"Model Parameters:\")\nprint(model4.get_params())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\n\n# List of models to evaluate\nmodels = [\n    (\"Logistic Regression\", LogisticRegression(random_state=9)),\n    (\"Ridge Classifier\", RidgeClassifier(random_state=9)),\n    (\"SGD Classifier\", SGDClassifier(random_state=9)),\n    (\"Support Vector Classifier (SVC)\", SVC(random_state=9)),\n    (\"Linear SVC\", LinearSVC(random_state=9)),\n    (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=9)),\n    (\"Random Forest\", RandomForestClassifier(random_state=9)),\n    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=9)),\n    (\"AdaBoost\", AdaBoostClassifier(random_state=9)),\n    (\"Extra Trees\", ExtraTreesClassifier(random_state=9)),\n    (\"XGBoost\", XGBClassifier(use_label_encoder=False, random_state=9, eval_metric='logloss')),\n    (\"Naive Bayes\", GaussianNB())\n]\n\n# Initialize Stratified K-Fold cross-validation\nskf_meta = StratifiedKFold(n_splits=4, shuffle=True, random_state=9)\n\n# Iterate through each model\nfor name, model7 in models:\n    # Generate out-of-fold predictions\n    meta_pred_oof = cross_val_predict(model7, X_meta, y_meta, cv=skf_meta, method='predict')\n\n    # Calculate MCC for the model\n    mcc = matthews_corrcoef(y_meta, meta_pred_oof)\n\n    # Print the model name and MCC score\n    print(f\"{name}: Matthews Correlation Coefficient (MCC) = {mcc:.5f}\")\n\n    # Train the model on the entire dataset\n    model7.fit(X_meta, y_meta)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier\nimport gc\n\n# Initialize OOF predictions array\noof_preds_catboost = np.zeros(len(X))  # assuming X is your dataset\n\n# List of saved model paths\nmodel_paths = [\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_0.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_1.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_2.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_3.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_4.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_5.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_6.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_7.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_8.cbm\",\n    \"/kaggle/input/catboost-oofs-and-best-model/catboost_model_fold_9.cbm\"\n]\n\n# KFold setup\ncv = KFold(n_splits=10, shuffle=True, random_state=9)\n\nfor fold, (train_ind,valid_ind) in enumerate(cv.split(X, y)):\n    # Load the saved model for the current fold\n    model = CatBoostClassifier()\n    model.load_model(model_paths[fold])\n    X_val = X.iloc[valid_ind]\n    # Generate OOF predictions for the validation set\n    oof_preds_catboost[valid_ind] = model.predict_proba(X_val)[:, 1]\n    # Clean up\n    gc.collect()\n\n# The array `oof_preds_catboost` now contains the OOF predictions for all validation sets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_meta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}